{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed-ups with single-precision floating points\n",
    "\n",
    "Using single precision floating points can improve the speed of training the models by atleast a factor of 2. The tradeoff is a small loss of accuracy and non-deterministic behavior due to rounding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from lvgp_pytorch.models import LVGPR\n",
    "from lvgp_pytorch.optim import fit_model_scipy,noise_tune\n",
    "from lvgp_pytorch.utils.variables import NumericalVariable,CategoricalVariable\n",
    "from lvgp_pytorch.utils.input_space import InputSpace\n",
    "\n",
    "from typing import Dict\n",
    "from copy import deepcopy\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the training and test sets\n",
    "\n",
    "We will be using the piston function for demonstration. The piston function is given by\n",
    "\n",
    "$$\n",
    "2\\pi\\sqrt{\\frac{M}{\n",
    "    k+ S^2\\frac{P_0V_0}{V^2}\\frac{T}{T_o}}},\n",
    "$$\n",
    "\n",
    "where $V = \\frac{S}{2k}\\sqrt{A^2+4k\\frac{P_0}{T_0}T}$,  $A = P_0S + 19.62 M - \\frac{kV_0}{S}$, and the 7 inputs are ($M,S,V_0,k,P_o,$ $T,T_0$). All 7 inputs are numerical. Similar to [Zhang et al. (2020)](https://doi.org/10.1080/00401706.2019.1638834), we discretize $P_0$ and $k$ over their domains to have 5 and 9 levels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Input space with variables:\n",
       "\n",
       "M, Type: Numerical, Range: [30.0,60.0]\n",
       "S, Type: Numerical, Range: [0.005,0.02]\n",
       "V_0, Type: Numerical, Range: [0.002,0.01]\n",
       "T_a, Type: Numerical, Range: [290.0,296.0]\n",
       "T_0, Type: Numerical, Range: [340.0,360.0]\n",
       "P_0, Type: Categorical, Levels: {90000.0, 95000.0, 100000.0, 105000.0, 110000.0}\n",
       "k, Type: Categorical, Levels: {1000.0, 1500.0, 2000.0, 2500.0, 3000.0, 3500.0, 4000.0, 4500.0, 5000.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reponse function\n",
    "def piston(params:Dict)->float:\n",
    "    A = params['P_0']*params['S'] + 19.62*params['M'] - params['k']*params['V_0']/params['S']\n",
    "    term1 = params['P_0']*params['V_0']/params['T_0']*params['T_a']\n",
    "    V = params['S']/2/params['k']*(math.sqrt(A**2 + 4*term1)-A)\n",
    "    term2 = params['k'] + (params['S']**2)*term1/(V**2)\n",
    "    return 2*math.pi*math.sqrt(params['M']/term2)\n",
    "\n",
    "# InputSpace object\n",
    "config = InputSpace()\n",
    "M = NumericalVariable(name='M',lower=30,upper=60)\n",
    "S = NumericalVariable(name='S',lower=0.005,upper=0.02)\n",
    "V0 = NumericalVariable(name='V_0',lower=0.002,upper=0.01)\n",
    "Ta = NumericalVariable(name='T_a',lower=290,upper=296)\n",
    "T0 = NumericalVariable(name='T_0',lower=340,upper=360)\n",
    "\n",
    "P0 = CategoricalVariable(name='P_0',levels=np.linspace(90000,110000,5))\n",
    "k = CategoricalVariable(name='k',levels=np.linspace(1000,5000,9))\n",
    "config.add_inputs([M,S,V0,Ta,T0,P0,k])\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate 500 random samples to be used as training data and separately 1000 random samples to be used as test data. Note that data to be supplied to the model for training and predictions are in double precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 500 samples\n",
    "set_seed(1)\n",
    "num_samples = 500\n",
    "train_x = torch.from_numpy(\n",
    "    config.random_sample(np.random,num_samples)\n",
    ")\n",
    "train_y = [None]*num_samples\n",
    "\n",
    "for i,x in enumerate(train_x):\n",
    "    train_y[i] = piston(config.get_dict_from_array(x.numpy()))\n",
    "\n",
    "train_y = torch.tensor(train_y).double()\n",
    "\n",
    "\n",
    "# generate 1000 test samples\n",
    "num_samples = 1000\n",
    "test_x = torch.from_numpy(config.random_sample(np.random,num_samples))\n",
    "test_y = [None]*num_samples\n",
    "\n",
    "for i,x in enumerate(test_x):\n",
    "    test_y[i] = piston(config.get_dict_from_array(x.numpy()))\n",
    "    \n",
    "# create tensor objects\n",
    "test_y = torch.tensor(test_y).to(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with double-precision\n",
    "\n",
    "\n",
    "To create a model instance with double precision, use the `.double()` method. Test data for which predictions are to be computed need to be in double precision format as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL for double-precision model...: -113.811\n",
      "Training time (seconds)..........: 208.79\n",
      "Test RRMSE.......................:  0.202\n"
     ]
    }
   ],
   "source": [
    "# create LVGP instance\n",
    "set_seed(4)\n",
    "model = LVGPR(\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    qual_index=config.qual_index,\n",
    "    quant_index=config.quant_index,\n",
    "    num_levels_per_var=list(config.num_levels.values()),\n",
    "    quant_correlation_class=\"RBFKernel\",\n",
    ").double()\n",
    "\n",
    "# fit model with 10 different starts\n",
    "start_time = time.time()\n",
    "_,nll_inc = fit_model_scipy(\n",
    "    model,\n",
    "    num_restarts=9, # number of starting points\n",
    ")\n",
    "fit_time = time.time()-start_time\n",
    "\n",
    "print('NLL for double-precision model...: %6.3f'%nll_inc)\n",
    "print('Training time (seconds)..........: %6.2f'%fit_time)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # set return_std = False if standard deviation is not needed\n",
    "    # test_x needs to be double precision \n",
    "    test_mean = model.predict(test_x,return_std=False)\n",
    "    \n",
    "# print RRMSE\n",
    "rrmse = torch.sqrt(((test_y-test_mean)**2).mean()/((test_y-test_y.mean())**2).mean())\n",
    "print('Test RRMSE.......................: %6.3f'%rrmse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with single-precision\n",
    "\n",
    "\n",
    "To create a model instance with double precision, use the `.single()` method. Test data for which predictions are to be computed need to be in single precision format as well. For a fair comparison, we use the same starting points for multi-start optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL for single-precision model...: -113.534\n",
      "Training time (seconds)..........:  54.35\n",
      "Test RRMSE.......................:  0.203\n"
     ]
    }
   ],
   "source": [
    "# create LVGP instance\n",
    "set_seed(4)\n",
    "model_float = LVGPR(\n",
    "    train_x=train_x,\n",
    "    train_y=train_y,\n",
    "    qual_index=config.qual_index,\n",
    "    quant_index=config.quant_index,\n",
    "    num_levels_per_var=list(config.num_levels.values()),\n",
    "    quant_correlation_class=\"RBFKernel\",\n",
    ").float() # changing this to float to use single-precision floating point\n",
    "\n",
    "# fit model with 10 different starts\n",
    "start_time = time.time()\n",
    "_,nll_inc_float = fit_model_scipy(\n",
    "    model_float,\n",
    "    num_restarts=9, # number of starting points\n",
    ")\n",
    "fit_time_float = time.time()-start_time\n",
    "\n",
    "print('NLL for single-precision model...: %6.3f'%nll_inc_float)\n",
    "print('Training time (seconds)..........: %6.2f'%fit_time_float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # set return_std = False if standard deviation is not needed\n",
    "    # since test_x is in double precisions, supply a single precision\n",
    "    # version test_x.float()\n",
    "    test_mean = model_float.predict(test_x.float(),return_std=False)\n",
    "    \n",
    "# print RRMSE\n",
    "rrmse_float = torch.sqrt(\n",
    "    ((test_y-test_mean.double())**2).mean()/((test_y-test_y.mean())**2).mean()\n",
    ")\n",
    "print('Test RRMSE.......................: %6.3f'%rrmse_float.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model single precision model takes about a quarter of the training time of the double precision model to train with a minor loss in accuracy. Note that due to the non-deterministic behavior due to rounding errors, the results obtained may slightly differ in different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lvgp-dev",
   "language": "python",
   "name": "lvgp-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
